{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "wine=datasets.load_wine()\n",
    "type(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.keys())\n",
    "print(wine.data)\n",
    "print(wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(\n",
    "    wine.data, wine.target, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_train))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(x_train, y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://miro.medium.com/max/1328/1*6Vy8J9kL_iXZeAh5KrZgCw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$precission=\\dfrac{TP}{TP+FP}$$\n",
    "$$recall=\\dfrac{TP}{TP+FN}$$\n",
    "$$f1-score=\\dfrac{recall+precission}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clima=pd.read_csv(\"Summary of Weather.csv\")\n",
    "clima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clima.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clima=clima.drop(['WindGustSpd', 'DR', 'SPD', 'FT','FB','FTI','ITH',\n",
    "                  'PGT','SD3','RHX','RHN','RVG','WTE', 'PoorWeather',\n",
    "                  'MaxTemp','MinTemp','MeanTemp','SND','TSHDSBRSGF',\n",
    "                  'Precip', 'Snowfall','SNF'], axis=1)\n",
    "clima=clima.dropna()\n",
    "clima=clima[clima['PRCP']!='T']\n",
    "clima['Date']=clima['Date'].astype('datetime64')\n",
    "clima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=clima['MAX'].values.reshape(-1, 1)\n",
    "y=clima['MIN'].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(\n",
    "    x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg=LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "prediction_space = np.linspace(min(x_train),\n",
    "                               max(x_train)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train, color='blue')\n",
    "plt.plot(prediction_space, reg.predict(prediction_space),\n",
    "         color='black', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(y_pred-y_test), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_results = cross_val_score(reg, x, y, cv=5)\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=clima.drop(['MIN', 'Date'],axis=1)\n",
    "names=list(x.columns)\n",
    "x=x.values\n",
    "y=clima['MIN'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RL+\\alpha\\sum ^n_{i=1}a_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "ridge = Ridge(normalize=True)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "for alpha in np.logspace(-4, 0, 50):\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge,x,y,cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.logspace(-4, 0, 50), ridge_scores)\n",
    "plt.plot(np.logspace(-4, 0, 50), ridge_scores_std, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RL-\\alpha\\sum ^n_{i=1}|a_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso_coef = lasso.fit(x, y).coef_\n",
    "\n",
    "plt.plot(range(len(names)), lasso_coef)\n",
    "plt.xticks(range(len(names)), names, rotation=60)\n",
    "plt.ylabel('Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diabetes=pd.read_csv('diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X=diabetes.drop(['Outcome'], axis=1)\n",
    "y=diabetes['Outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4, random_state=42)\n",
    "\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# probabilidades predecidas\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "#false positive rate, true positive rate, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area bajo roc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizacion de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "logreg_cv.fit(X,y)\n",
    "\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Randomized Search es peor pero mas rapido\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# No veremos arboles, les recomiendo buscarlos\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "tree_cv.fit(X,y)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, como entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4, random_state=42)\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laso es l1, ridge es l2\n",
    "l1_ratio=1 es l2=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preposecing\n",
    "Pandas:get_dummies()\n",
    "scikit-learn:OneHotEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('mushrooms.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class']=df['class'].replace(('p', 'e'),(1, 0))\n",
    "df['gill-attachment']=df['gill-attachment'].replace(('a', 'f'),(1, 0))\n",
    "df['bruises']=df['bruises'].replace(('t', 'f'),(1, 0))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dfn=df._get_numeric_data()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "X=dfn.drop(['class'], axis=1)\n",
    "y=dfn['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables: df_region\n",
    "df_dum = pd.get_dummies(df)\n",
    "\n",
    "# Print the columns of df_region\n",
    "print(df_dum.columns)\n",
    "\n",
    "# Create dummy variables with drop_first=True: df_region\n",
    "df_dum2 = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Print the new columns of df_region\n",
    "print(df_dum2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dfn=df_dum._get_numeric_data()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "X=dfn.drop(['class'], axis=1)\n",
    "y=dfn['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dfn=df_dum2._get_numeric_data()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "X=dfn.drop(['class'], axis=1)\n",
    "y=dfn['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4, random_state=42)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como lidear con datos vacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['SkinThickness','Insulin']]=df[['SkinThickness','Insulin']].replace(0,np.nan)\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"Shape of Original DataFrame: {}\".format(df.shape))\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(['Outcome'],axis=1)\n",
    "y=df['Outcome']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=42)\n",
    "\n",
    "svc=SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('diabetes.csv')\n",
    "df[['SkinThickness','Insulin']]=df[['SkinThickness','Insulin']].replace(0,np.nan)\n",
    "X=df.drop(['Outcome'],axis=1)\n",
    "y=df['Outcome']\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "steps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),('SVM', SVC())]\n",
    "#Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=42)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X_scaled=X.dropna()\n",
    "X_scaled['Insulin'] = scale(X_scaled['Insulin'])\n",
    "\n",
    "print(\"Mean of Unscaled Features: {}\".format(np.mean(X.dropna()))) \n",
    "print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X.dropna())))\n",
    "\n",
    "print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \n",
    "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp=SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp.fit(X)\n",
    "X_imp=imp.transform(X)\n",
    "\n",
    "X_scaled = scale(X)\n",
    "\n",
    "print(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \n",
    "print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n",
    "\n",
    "print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \n",
    "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('diabetes.csv')\n",
    "X=df.drop(['Outcome'],axis=1)\n",
    "y=df['Outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('scaler', StandardScaler()),('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.3,random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training set: knn_scaled\n",
    "knn_scaled = pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Instantiate and fit a k-NN classifier to the unscaled data\n",
    "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test,y_test)))\n",
    "print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('scaler', StandardScaler()),\n",
    "         ('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters, cv=3)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('diabetes.csv')\n",
    "\n",
    "df[['SkinThickness','Insulin', 'BloodPressure','BMI','Glucose']]=df[['SkinThickness','Insulin', 'BloodPressure','BMI','Glucose']].replace(0,np.nan)\n",
    "\n",
    "X=df.drop(['Outcome'],axis=1)\n",
    "y=df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the pipeline steps: steps\n",
    "steps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='mean')),('scaler', StandardScaler()),('knn', KNeighborsClassifier())]\n",
    "\n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'knn__n_neighbors':range(0,30)}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4,random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(pipeline,parameters, cv=3)\n",
    "\n",
    "# Fit to the training set\n",
    "gm_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"Tuned KNN n_neighbors: {}\".format(gm_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "steps = [('imputation', SimpleImputer(missing_values=np.nan, strategy='mean')),('scaler', StandardScaler()),('knn', KNeighborsClassifier(n_neighbors=25))]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
